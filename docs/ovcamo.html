<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="Homepage" />
  <meta name="keywords" content="Homepage" />
  <meta name="author" content="Lart Pang" />
  <title>OVCamo-ECCV2024</title>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@1.0.1/css/bulma.min.css">
  <link rel="apple-touch-icon" href="apple-touch-icon.png" />
  <link rel="icon" href="favicon.ico" />
  <style>
    body {
      font-family: Georgia, serif;
    }
  </style>
</head>

<body>
  <nav class="navbar is-white" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://lartpang.github.io">
        <svg t="1723627295537" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="13432" width="200" height="200">
          <path d="M484.32 375.24C575.25 255.5 857.87 527.6 788.67 581.07c-94.76 73.21-491.01 39.99-304.35-205.83z" fill="#1C80AA" p-id="13433"></path>
          <path d="M401.03 749.89l-4.85 133.8-77.69 21.37h66.36l19.42 35.27 4.86-35.27 40.46 6.14-38.84-25.89 8.09-114.91-17.81-20.51zM524.36 771.23l10.48 133.48-74.73 30.11 65.92-7.59 23.33 32.82 0.79-35.6 40.89 1.48-41.54-21.28-5.11-115.08-20.03-18.34z" fill="#3B5174" p-id="13434"></path>
          <path d="M224.73 264.77l-24 50.19a21.7 21.7 0 0 1-37.73 2.5l-31.57-48.27a21.7 21.7 0 0 1 17.41-33.57l55.61-1.92a21.7 21.7 0 0 1 20.28 31.07z" fill="#DE7B56" p-id="13435"></path>
          <path d="M900.53 638.76c-18.3 86.91-221.86 208.13-478 171.54C150.46 771.44 26 281.88 315 103.56c161.25-99.49 326.71 5 356.8 130.37C713 405.47 583.15 534.58 749.57 609c86.91 38.91 164.43-34.33 150.96 29.76z" fill="#FDD2BE" p-id="13436"></path>
          <path d="M365.86 264.78m-32.45 0a32.45 32.45 0 1 0 64.9 0 32.45 32.45 0 1 0-64.9 0Z" fill="" p-id="13437"></path>
          <path d="M512.24 366c137.48-60.86 253.34 314 166.92 327.31C560.81 711.56 230 490.92 512.24 366zM223.3 530c-9.34-2.6-17.2-12.8-23.94-31a195 195 0 0 1-7.64-27 7.28 7.28 0 0 1 14.3-2.79c4.79 24.5 15 46.44 21.91 46.93 1.12 0.08 11.43-0.5 27.23-45.51a7.28 7.28 0 1 1 13.74 4.82c-13.61 38.77-27 56.31-42 55.22a18.18 18.18 0 0 1-3.6-0.67zM340.8 590.36c-9.63 1.14-20.77-5.32-33.92-19.63a195 195 0 0 1-17.32-22.11 7.28 7.28 0 0 1 12.17-8c13.73 20.85 31.53 37.27 38.07 35.12 1.07-0.35 10.38-4.8 7.93-52.44a7.28 7.28 0 1 1 14.55-0.75c2.11 41-3.59 62.33-17.95 67a18.18 18.18 0 0 1-3.53 0.81zM261.5 659.71c-9-0.19-18.35-7.55-28.56-22.35a180.41 180.41 0 0 1-13-22.49 6.74 6.74 0 0 1 12.18-5.77c9.9 20.88 24.1 38.21 30.37 37.08 1-0.18 10.13-3.07 14-47a6.74 6.74 0 1 1 13.43 1.18c-3.34 37.87-11.31 56.66-25.07 59.12a16.82 16.82 0 0 1-3.35 0.23zM389.28 722.29c-9.26 2.85-21.38-1.51-36.89-13.22a195 195 0 0 1-21-18.64 7.28 7.28 0 0 1 10.53-10.06c17.25 18.05 37.7 31 43.75 27.71 1-0.54 9.35-6.59-1.61-53a7.28 7.28 0 1 1 14.17-3.35c9.44 40 7.65 62-5.63 69.16a18.18 18.18 0 0 1-3.32 1.4z" fill="#22B0C3" p-id="13438"></path>
        </svg>
      </a>

      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbar_">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>

    <div id="navbar_" class="navbar-menu">
      <div class="navbar-start">
        <a class="navbar-item" href="#abstract">üëã Abstract</a>
        <a class="navbar-item" href="#dataset">üöô OVCamo</a>
        <a class="navbar-item" href="#method">üìñ OVCoser</a>
      </div>

    </div>
  </nav>

  <section class="section has-background-white-ter">
    <div class="container">
      <div class="columns">

        <div class="column">

          <div class="card" id="banner">
            <div class="card-image">
              <figure class="image is-3by1">
                <img src="./assets/ovcamo/teaser.jpg" alt="teaser" />
              </figure>
            </div>

            <div class="card-content">
              <div class="media">
                <div class="media-content">
                  <div class="has-text-centered">
                    <span class="title is-3">OVCOS: Open-Vocabulary Camouflaged Object Segmentation</span>
                    <br />
                    <span class="paper-author"><span class="author-me">Youwei Pang</span>*, Xiaoqi Zhao*, Jiaming Zuo, Lihe Zhang, Huchuan Lu</span>.
                    <br />
                    <span class="paper-conf">European Conference on Computer Vision (ECCV) 2024, Springer</span>
                  </div>

                  <div class="tags has-text-weight-bold is-centered">
                    <span class="tag is-light">[<a href="https://arxiv.org/abs/2311.11241" target="_blank">üìÇ ArXiv</a>]</span>
                    <span class="tag is-light">[<a href="https://github.com/lartpang/OVCamo" target="_blank">üêô GitHub (Code & Dataset)</a>]</span>
                  </div>

                  <details>
                    <summary>üñêÔ∏è BibTeX (<code>OVCOS-ECCV2024</code>)</summary>
                    <pre>@inproceedings{OVCOS-ECCV2024,
  title={Open-Vocabulary Camouflaged Object Segmentation},
  author={Pang, Youwei and Zhao, Xiaoqi and Zuo, Jiaming and Zhang, Lihe and Lu, Huchuan},
  booktitle=ECCV,
  year={2024},
}</pre>
                  </details>
                </div>
              </div>
            </div>
          </div>

          <div class="box" id="abstract">
            <p class="title is-4">üëã Abstract</p>

            <p>
              Recently, the emergence of the large-scale vision-language model (VLM), such as CLIP, has opened the way towards open-world object perception.
              Many works have explored the utilization of pre-trained VLM for the challenging open-vocabulary dense prediction task that requires perceiving diverse objects with novel classes at inference time.
              Existing methods construct experiments based on the public datasets of related tasks, which are not tailored for open vocabulary and rarely involve imperceptible objects camouflaged in complex scenes due to data collection bias and annotation costs.
              To fill in the gaps, we introduce a new task, <strong>open-vocabulary camouflaged object segmentation (OVCOS)</strong>, and construct <strong>a large-scale complex scene dataset (OVCamo)</strong> containing 11,483 hand-selected images with fine annotations and corresponding object classes.
              Further, we build <strong>a strong single-stage open-vocabulary camouflaged object segmentation transformer baseline OVCoser</strong> attached to the parameter-fixed CLIP with iterative semantic guidance and structure enhancement.
              By integrating the guidance of class semantic knowledge and the supplement of visual structure cues from the edge and depth information, the proposed method can efficiently capture camouflaged objects.
              Moreover, this effective framework also surpasses previous state-of-the-arts of open-vocabulary semantic image segmentation by a large margin on our OVCamo dataset.
              With the proposed dataset and baseline, we hope that this new task with more practical value can further expand the research on open-vocabulary dense prediction tasks.
            </p>
          </div>

          <div class="box" id="dataset">
            <p class="title is-4">üöô OVCamo</p>

            <div class="columns is-centered">
              <div class="column">
                <figure class="image">
                  <img src="https://arxiv.org/html/2311.11241v3/x2.png" />
                  <figcaption class="caption">Class distribution in the proposed OVCamo dataset and some visual examples.</figcaption>
                </figure>
              </div>
            </div>

            <p>
              Our data is collected from existing CSU datasets that have finely annotated segmentation maps.
              Specifically, the OVCamo integrates 11,483 hand-selected images covering 75 object classes reconstructed from several public datasets.
              The distribution of the number of samples in different classes is shown in this figure.
            </p>

            <div class="columns is-centered">
              <div class="column is-three-quarters">
                <figure class="image">
                  <img src="./assets/ovcamo/attr_vis.png" />
                  <figcaption class="caption">Class distribution in the proposed OVCamo dataset and some visual examples.</figcaption>
                </figure>
              </div>
            </div>

            <p>
              Meanwhile, we consider attributes of objects when selecting images, such as object concentration, average color ratio, object-image area ratio, number of object parts, and normalized centroid.
              This figure visualizes the attribute distribution of the proposed dataset.
              The camouflaged objects of interest usually have complex shape (a), high similarity to the background (b), and small size (c).
              And the image often contains multiple camouflaged objects or sub-regions with a central bias as shown in (d) and (e).
            </p>

            <div class="columns is-centered">
              <div class="column is-half">
                <figure class="image">
                  <img src="https://arxiv.org/html/2311.11241v3/x17.png" />
                  <figcaption class="caption">
                    Hierarchy of sample classes contained in the proposed OVCamo.
                    Only the base classes are used in our experiments.
                    Sub-classes that do not meet the criteria are simply removed and are not displayed here.
                  </figcaption>
                </figure>
              </div>
            </div>

            <p>
              We use the alluvial graph to show the class relationships at different levels, including super, base, and sub-classes.
              The sub-classes shown here include class names with clearer meanings preserved from the original data and class names after initial manual correction.
              The base classes represent the class names obtained after careful manual filtering and merging, which was used in all the experiments in this paper.
              The super classes generalize the base classes from a broader perspective.
            </p>
          </div>

          <div class="box" id="method">
            <p class="title is-4">üìñ OVCoser</p>

            <div class="block" id="model">
              <p class="title is-5">Model</p>

              <div class="columns is-centered">
                <div class="column is-half">
                  <figure class="image">
                    <img src="https://arxiv.org/html/2311.11241v3/x1.png" />
                    <figcaption class="caption">The perception and recognition of camouflaged objects require the collaboration of information from multiple sources such as appearance clues, spatial structure, contour details, and object semantics.</figcaption>
                  </figure>
                </div>
              </div>

              <p>
                The camouflage arises from several sources, including similar patterns to the environment (e.g., color and texture) and imperceptible attributes (e.g., small size and heavy occlusion).
                Considering the imperceptible appearance of camouflaged objects, accurate recognition and capture actually depend more on the cooperation of multi-source knowledge.
                As shown in these figures, in addition to visual appearance cues, we introduce the depth for the spatial structure of the scene, the edge for the regional changes about objects, and the text for the context-aware class semantics.
              </p>

              <div class="columns is-centered">
                <div class="column is-two-thirds">
                  <figure class="image">
                    <img src="https://arxiv.org/html/2311.11241v3/x8.png" />
                    <figcaption class="caption">Overview of our single-stage open-vocabulary camouflaged object segmentation framework, OVCoser.</figcaption>
                  </figure>
                </div>
              </div>

              <p>
                We design a strong baseline OVCoser for the proposed OVCOS, based on the VLM-driven single-stage paradigm.
                Considering the cooperative relationship between class recognition and object perception, the iterative learning strategy is introduced to feed back the optimized semantic relationship, resulting in more accurate object semantic guidance.
                This top-down conceptual reinforcement can further optimize open-vocabulary segmentation performance.
                With the help of the iterative multi-source information joint learning strategy, our method OVCoser shows good performance in the proposed OVCOS task.
              </p>
            </div>

            <div class="block" id="experiment">
              <p class="title is-5">Experiment</p>

              <div class="columns is-centered">
                <div class="column is-two-thirds">
                  <figure class="image">
                    <img src="https://arxiv.org/html/2311.11241v3/x12.png" />
                    <figcaption class="caption">
                      Visual results on OVCamo.
                      Existing methods are either disrupted by chaotic backgrounds, imperceptible appearances, blurry details, or severe occlusion, while our algorithm can effectively capture and remain well-exposed object details.
                      And three different colors are used to represent
                      <span class="has-text-danger">human annotations</span>,
                      <span class="has-text-success">correct predictions</span>,
                      and <span class="has-text-info">incorrect predictions</span>.
                    </figcaption>
                  </figure>
                </div>
              </div>

              <p>
                We also visualize the results of some recent methods on a variety of data in this figure.
                It can be seen that the proposed method shows better performance and adaptability to diverse objects, including
                large objects (Col. 1-2),
                middle objects (Col. 3-5),
                small objects (Col. 6-9),
                multiple objects (Col. 8),
                complex shapes (Col. 3-5),
                blurred edges (Col. 1-5),
                severe occlusion (Col. 6),
                and background interference (Col. 2-6).
              </p>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Footer -->
  <footer class="footer py-4">
    <div class="content has-text-centered">
      Last updated at <time datetime="YYYY-MM-DD">2024-09-03</time>
      ¬© 2024 Youwei Pang. All rights reserved.
    </div>
  </footer>
</body>

<script>
  // ÂÆö‰πâÊò†Â∞ÑÂÖ≥Á≥ª
  const classMap = {
    'paper-title': ['has-text-weight-bold'],
    'paper-author': ['is-italic'],
    'paper-conf': ['has-text-weight-semibold'],
    'author-me': ['has-text-info', 'has-text-weight-bold'],
    'tag-link': ['tag', 'is-light', 'has-text-weight-bold'],
    'caption': ['is-italic', 'has-text-centered']
  };

  // ÊõøÊç¢Á±ªÂà´Âêç‰∏∫ÂÆûÈôÖÁöÑÁ±ªÂÆö‰πâ
  function replaceClassAliases() {
    // ÈÅçÂéÜÊò†Â∞Ñ‰∏≠ÁöÑÊØè‰∏™Âà´Âêç
    Object.keys(classMap).forEach(alias => {
      // Êü•Êâæ‰ΩøÁî®Âà´ÂêçÁöÑÊâÄÊúâÂÖÉÁ¥†
      document.querySelectorAll('.' + alias).forEach(element => {
        // ÁßªÈô§Âà´ÂêçÁ±ª
        element.classList.remove(alias);
        // Ê∑ªÂä†Êò†Â∞ÑÂà∞ÁöÑÂÆûÈôÖÁ±ª
        classMap[alias].forEach(actualClass => {
          element.classList.add(actualClass);
        });
      });
    });
  }

  // navbar active, https://bulma.io/documentation/components/navbar/#navbar-menu
  function activateNavbarMenu() {
    // Get all "navbar-burger" elements
    const $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);
    // Add a click event on each of them
    $navbarBurgers.forEach(el => {
      el.addEventListener('click', () => {
        // Get the target from the "data-target" attribute
        const target = el.dataset.target;
        const $target = document.getElementById(target);
        // Toggle the "is-active" class on both the "navbar-burger" and the "navbar-menu"
        el.classList.toggle('is-active');
        $target.classList.toggle('is-active');
      });
    });
  }

  // Âú®ÊñáÊ°£Âä†ËΩΩÂÆåÊàêÂêéÊâßË°åÊõøÊç¢
  document.addEventListener('DOMContentLoaded', () => {
    replaceClassAliases();
    activateNavbarMenu();
  });
</script>

</html>
