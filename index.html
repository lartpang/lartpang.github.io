<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="Homepage" />
  <meta name="keywords" content="Homepage" />
  <meta name="author" content="Lart Pang" />
  <title>Youwei Pang</title>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@1.0.4/css/bulma.min.css">
  <link rel="apple-touch-icon" href="apple-touch-icon.png" />
  <link rel="icon" href="favicon.ico" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Zhi+Mang+Xing&display=swap" rel="stylesheet">
  <style>
    .handwriting-text {
      font-family: "Zhi Mang Xing", cursive;
      font-weight: 400;
      font-style: normal;
    }

    body {
      font-family: Georgia, "Times New Roman", Times, serif;
      color: #333;
    }

    /* ËÆæÁΩÆÊªöÂä®ÂÆπÂô® */
    .scrollable-box {
      height: 12rem;
      overflow-y: auto;
      border: 1px solid #ccc;
      border-radius: 10px;
      padding: 10px;
    }

    .card {
      padding: 0.3rem 0.5rem 0.3rem 0.5rem;
      box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
      transition: box-shadow .2s;
    }

    .card:hover {
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
    }

    /* ËÆæÁΩÆÊñ∞ÈóªÂàóË°® */
    .scrollable-list {
      list-style: none;
      padding: 0;
      margin: 0;
    }

    /* ÊØè‰∏™Êñ∞ÈóªÊù°ÁõÆÁöÑÊ†∑Âºè */
    .scrollable-item {
      display: flex;
      margin: 0 0 0.25rem 0.25rem;
      /* ‰∏äËæπ | Âè≥Ëæπ | ‰∏ãËæπ | Â∑¶Ëæπ */
      align-items: flex-start;
    }

    /* Êó∂Èó¥ÈÉ®ÂàÜ */
    .info-date {
      flex-shrink: 0;
      width: 6rem;
      margin-right: 0.5rem;
      font-weight: bold;
    }

    /* ÂÜÖÂÆπÈÉ®ÂàÜ */
    .info-content {
      word-wrap: break-word;
      /* ÂÖÅËÆ∏ÂÜÖÂÆπÊç¢Ë°å */
      flex-grow: 1;
      /* ÂÜÖÂÆπÈÉ®ÂàÜÂ°´Êª°Ââ©‰ΩôÁ©∫Èó¥ */
    }

    #email {
      cursor: pointer;
      font-family: monospace;
      color: blue;
      text-decoration: underline;
    }

    #banner {
      position: relative;
      overflow: hidden;
      background: linear-gradient(135deg, #FDD2BE 0%, #84899b 50%, #1C80AA 100%);
      align-items: center;
    }

    .hero-content {
      position: relative;
      display: flex;
      z-index: 2;
    }

    /* Á≤íÂ≠êÁîªÂ∏É */
    #particles-canvas {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      z-index: 1;
    }

    .profile-image {
      border: 4px solid white;
      transition: transform 0.3s ease;
      will-change: transform;
      /* ÊÄßËÉΩ‰ºòÂåñ */
    }

    .profile-image:hover {
      transform: scale(1.05);
    }

  </style>
</head>

<body>
  <nav class="navbar is-white" role="navigation" aria-label="main navigation">
    <div class="container">

      <div class="navbar-brand">
        <a class="navbar-item" href="#">
          <svg t="1723627295537" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="13432" width="200" height="200">
            <path d="M484.32 375.24C575.25 255.5 857.87 527.6 788.67 581.07c-94.76 73.21-491.01 39.99-304.35-205.83z" fill="#1C80AA" p-id="13433"></path>
            <path d="M401.03 749.89l-4.85 133.8-77.69 21.37h66.36l19.42 35.27 4.86-35.27 40.46 6.14-38.84-25.89 8.09-114.91-17.81-20.51zM524.36 771.23l10.48 133.48-74.73 30.11 65.92-7.59 23.33 32.82 0.79-35.6 40.89 1.48-41.54-21.28-5.11-115.08-20.03-18.34z" fill="#3B5174" p-id="13434"></path>
            <path d="M224.73 264.77l-24 50.19a21.7 21.7 0 0 1-37.73 2.5l-31.57-48.27a21.7 21.7 0 0 1 17.41-33.57l55.61-1.92a21.7 21.7 0 0 1 20.28 31.07z" fill="#DE7B56" p-id="13435"></path>
            <path d="M900.53 638.76c-18.3 86.91-221.86 208.13-478 171.54C150.46 771.44 26 281.88 315 103.56c161.25-99.49 326.71 5 356.8 130.37C713 405.47 583.15 534.58 749.57 609c86.91 38.91 164.43-34.33 150.96 29.76z" fill="#FDD2BE" p-id="13436"></path>
            <path d="M365.86 264.78m-32.45 0a32.45 32.45 0 1 0 64.9 0 32.45 32.45 0 1 0-64.9 0Z" fill="" p-id="13437"></path>
            <path d="M512.24 366c137.48-60.86 253.34 314 166.92 327.31C560.81 711.56 230 490.92 512.24 366zM223.3 530c-9.34-2.6-17.2-12.8-23.94-31a195 195 0 0 1-7.64-27 7.28 7.28 0 0 1 14.3-2.79c4.79 24.5 15 46.44 21.91 46.93 1.12 0.08 11.43-0.5 27.23-45.51a7.28 7.28 0 1 1 13.74 4.82c-13.61 38.77-27 56.31-42 55.22a18.18 18.18 0 0 1-3.6-0.67zM340.8 590.36c-9.63 1.14-20.77-5.32-33.92-19.63a195 195 0 0 1-17.32-22.11 7.28 7.28 0 0 1 12.17-8c13.73 20.85 31.53 37.27 38.07 35.12 1.07-0.35 10.38-4.8 7.93-52.44a7.28 7.28 0 1 1 14.55-0.75c2.11 41-3.59 62.33-17.95 67a18.18 18.18 0 0 1-3.53 0.81zM261.5 659.71c-9-0.19-18.35-7.55-28.56-22.35a180.41 180.41 0 0 1-13-22.49 6.74 6.74 0 0 1 12.18-5.77c9.9 20.88 24.1 38.21 30.37 37.08 1-0.18 10.13-3.07 14-47a6.74 6.74 0 1 1 13.43 1.18c-3.34 37.87-11.31 56.66-25.07 59.12a16.82 16.82 0 0 1-3.35 0.23zM389.28 722.29c-9.26 2.85-21.38-1.51-36.89-13.22a195 195 0 0 1-21-18.64 7.28 7.28 0 0 1 10.53-10.06c17.25 18.05 37.7 31 43.75 27.71 1-0.54 9.35-6.59-1.61-53a7.28 7.28 0 1 1 14.17-3.35c9.44 40 7.65 62-5.63 69.16a18.18 18.18 0 0 1-3.32 1.4z" fill="#22B0C3" p-id="13438"></path>
          </svg>
        </a>

        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbar_">
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div>

      <div id="navbar_" class="navbar-menu">
        <div class="navbar-start">
          <a class="navbar-item" href="#about">üëã About Me</a>
          <a class="navbar-item" href="#news">üì¢ News</a>
          <a class="navbar-item" href="#resource">üìÇ Resource</a>
          <a class="navbar-item" href="#project">üíª Project</a>
          <a class="navbar-item" href="#paper-list">üìñ Paper</a>
        </div>
      </div>

    </div>
  </nav>

  <section class="hero is-fullheight-with-navbar" id="banner">
    <div class="hero-body">
      <canvas id="particles-canvas"></canvas>

      <div class="container hero-content">
        <div class="columns is-vcentered is-centered">
          <div class="column is-3">
            <figure class="image is-256x256">
              <img class="profile-image is-rounded" src="https://avatars.githubusercontent.com/u/26847524" alt="Avatar" />
            </figure>
          </div>

          <div class="column">
            <div class="block">
              <p>
                <span class="title is-1 has-text-white handwriting-text">Youwei Pang</span>
                <span class="title is-3 has-text-white">, Ph.D.</span>
              </p>
              <p class="title is-4 has-text-white">Computer Vision Researcher & AI Enthusiast</p>
            </div>

            <div class="buttons">
              <button class="button is-light">üìß <span id="email" onclick="startDecoding()">qfwyufsl</span> [at] Gmail</button>
              <button class="button is-light"><a href="https://scholar.google.com/citations?user=jdo9_goAAAAJ&hl=en" target="_blank">üìö Google Scholar</a></button>
              <button class="button is-light"><a href="https://orcid.org/0000-0002-3950-0956" target="_blank">üçè ORCID</a></button>
              <button class="button is-light"><a href="https://github.com/lartpang" target="_blank">üêô GitHub</a></button>
            </div>

            <div class="buttons">
              <button class="button is-light"><a href="https://blog.csdn.net/p_lart" target="_blank">‚úèÔ∏è CSDN</a></button>
              <button class="button is-light"><a href="https://www.cnblogs.com/lart" target="_blank">‚úèÔ∏è ÂçöÂÆ¢Âõ≠</a></button>
              <button class="button is-light"><a href="https://www.yuque.com/lart" target="_blank">‚úèÔ∏è ËØ≠ÈõÄ</a></button>
              <button class="button is-light"><a href="https://tuchong.com/11269264/posts/" target="_blank">üñºÔ∏è ÂõæËô´</a></button>
            </div>
          </div>
        </div>

      </div>
    </div>
  </section>

  <section class="section has-background-white-ter">
    <div class="container">
      <div class="columns">
        <div class="column is-10 is-offset-1">

          <div class="box" id="about">
            <p class="title is-4">üëã About Me</p>

            <div class="block">
              <p class="boldsubtitle">My Life</p>

              <p>
                üåé In my free time, I enjoy
                focused programming‚å®Ô∏è (current Github <img alt="GitHub User's stars" src="https://img.shields.io/github/stars/lartpang?style=plastic&logo=github&label=Stars&color=red">),
                <a href="https://www.yuque.com/lart/record/zf2tgt">patient readingüìö</a>,
                <a href="https://www.yuque.com/lart/record/ladtsq01s675c9g9">musical adventuresüéß</a>,
                fast runningüèÉ‚Äç‚ôÇÔ∏è,
                slow swimmingüèä‚Äç‚ôÇÔ∏è,
                city walkingüö∂‚Äç‚ôÇÔ∏è,
                and exploring delicious foodüç≤.
                <br />
                ü§ù I work very closely with my best friend, <span class="has-text-weight-bold"><a href="https://xiaoqi-zhao-dlut.github.io/">Xiaoqi Zhao</a></span>, on most of my projects.
              </p>
            </div>

            <div class="block">
              <p class="boldsubtitle">My Experience</p>

              <div class="media">
                <figure class="media-left">
                  <p class="image is-64x64">
                    <img src="https://www.wur.nl/upload/52cdf268-8d6c-4713-acaa-e9f93ff69632_Logo.jpg" />
                  </p>
                </figure>
                <div class="media-content">
                  <p>
                    <span class="subtitle is-6"><time class="info-date">2025-Present:</time>
                      <a href="https://www.ntu.edu.sg/">Nanyang Technological University (NTU)</a></span>
                    <br />
                    Research Fellow
                    <br />
                    Work with <a href="https://personal.ntu.edu.sg/wslin/">Prof. Weisi Lin (IEEE Fellow)</a>.
                  </p>
                </div>
              </div>

              <div class="media">
                <figure class="media-left">
                  <p class="image is-64x64">
                    <img src="https://www.dlut.edu.cn/__local/B/E6/D2/C96F6B930EC74F5666EDEA31D38_CAC1603C_6CB6.png" />
                  </p>
                </figure>
                <div class="media-content">
                  <p>
                    <span class="subtitle is-6"><time class="info-date">2019-2025:</time>
                      <a href="https://ice.dlut.edu.cn/index.htm">School of Information and Communication Engineering</a>,
                      <a href="https://www.dlut.edu.cn/">Dalian University of Technology (DUT)</a></span>
                    <br />
                    Completed my Ph.D. in Signal and Information Processing.
                    <br />
                    Supervised by <a href="http://faculty.dlut.edu.cn/lhzhang/zh_CN/index.htm">Prof. Lihe Zhang</a> and <a href="http://faculty.dlut.edu.cn/Huchuan_Lu/zh_CN/index.htm">Prof. Huchuan Lu (IEEE Fellow)</a>.
                  </p>
                </div>
              </div>

              <div class="media">
                <figure class="media-left">
                  <p class="image is-64x64">
                    <img src="https://www.dlut.edu.cn/__local/B/E6/D2/C96F6B930EC74F5666EDEA31D38_CAC1603C_6CB6.png" />
                  </p>
                </figure>
                <div class="media-content">
                  <p>
                    <span class="subtitle is-6"><time class="info-date">2015-2019:</time>
                      <a href="https://ice.dlut.edu.cn/index.htm">School of Information and Communication Engineering</a>,
                      <a href="https://www.dlut.edu.cn/">Dalian University of Technology (DUT)</a></span>
                    <br />
                    Received the B.E. in Electronic and Information Engineering.
                  </p>
                </div>
              </div>
            </div>

            <div class="block">
              <p class="boldsubtitle">Research Interest</p>

              <p>
                My current research interests include
                <span class="is-italic has-text-weight-bold">Deep Learning</span>,
                <span class="is-italic has-text-weight-bold">Computer Vision</span>,
                and <span class="is-italic has-text-weight-bold">Neural Network Design</span>.
              </p>

              <div class="content">
                In particular, I focus on:
                <ul>
                  <li><span class="has-text-weight-bold">Multi-Task Joint Modeling</span>:
                    <ul class="ml-4 mt-0">
                      <li><span class="has-text-weight-bold">Natural Scene</span>: Context-Dependent (CD) Concept Perception [<a href="#unified-spider">ICML'24</a>] (Salient, Camouflaged [<a href="#cod-zoomnext">TPAMI'24</a>], Shadow [<a href="#rawsd-sara">ICCV'23</a>], ...), Remote Sensing Image Analysis, Semantic Segmentation [<a href="#multimodal-caver">TPAMI'25</a>]</li>
                      <li><span class="has-text-weight-bold">Industrial Scene</span>: Quality Inspection (Lithium Battery, Semiconductor) [<a href="#pbd-conf">CVPR'24</a>,<a href="#pbd-jnl">ArXiv'24</a>]</li>
                      <li><span class="has-text-weight-bold">Medical Scene</span>: Lesion, Organ, Tissue, ... [<a href="#medical-m2snet">ArXiv'23</a>]</li>
                      <li><span class="has-text-weight-bold">General Architecture</span>: RGB + RGB/Depth/Thermal/Temporal [<a href="#cod-zoomnext">TPAMI'24</a>,<a href="#multimodal-caver">TPAMI'25</a>]</li>
                      <li><span class="has-text-weight-bold">Task-Generic Paradigm</span>: Unified Framework for CD Concept Understanding [<a href="#unified-spider">ICML'24</a>]</li>
                    </ul>
                  </li>
                  <li><span class="has-text-weight-bold">Multi-Modal Data Integration</span>:
                    <ul class="ml-4 mt-0">
                      <li><span class="has-text-weight-bold">Vision (Multi-Sensor)</span>: Modality-Complete [<a href="#multimodal-hdfnet">ECCV'20</a>,<a href="#multimodal-danet">ECCV'20</a>,<a href="#multimodal-jlsdc">TIP'22</a>,<a href="#multimodal-caver">TIP'23</a>,<a href="#multimodal-comptr">TPAMI'25</a>], Modality-Incomplete [<a href="#multimodal-unimerseg">NeurIPS'25</a>]</li>
                      <li><span class="has-text-weight-bold">Vision (Multi-View)</span>: Scale, Edge, ... [<a href="#sod-minet">CVPR'20</a>,<a href="#gatenet-conf">ECCV'20</a>,<a href="#gatenet-jnl">IJCV'24</a>,<a href="#dis-mvanet">CVPR'24</a>]</li>
                      <li><span class="has-text-weight-bold">Vision (Multi-Moment)</span>: Temporal Cues [<a href="#cod-zoomnext">TPAMI'24</a>]</li>
                      <li><span class="has-text-weight-bold">Vision-Language</span>: Open-Vocabulary [<a href="#cod-ovcos">ECCV'24</a>], Referring [<a href="#cod-cgcod">ACM MM'25</a>]</li>
                    </ul>
                  </li>
                  <li><span class="has-text-weight-bold">Efficient Model Design</span>:
                    <ul class="ml-4 mt-0">
                      <li><span class="has-text-weight-bold">Lightweight Operation</span>: Efficient Attention [<a href="#multimodal-caver">TIP'23</a>,<a href="#multimodal-comptr">TPAMI'25</a>], Dynamic Convolution [<a href="#multimodal-hdfnet">ECCV'20</a>]</li>
                      <li><span class="has-text-weight-bold">Scale-Generic Modeling</span>: Tiny Object Perception [<a href="#rethinking-irstd">NeurIPS'25</a>], Industrial Quality Inspection [<a href="#pbd-conf">CVPR'24</a>,<a href="#pbd-jnl">ArXiv'25</a>]</li>
                      <li><span class="has-text-weight-bold">Resolution Adaptability</span>: High Resolution Object Segmentation [<a href="#dis-mvanet">CVPR'24</a>]</li>
                    </ul>
                  </li>
                </ul>
              </div>
            </div>

          </div>

          <div class="box">
            <div class="columns">
              <div class="column is-6" id="news">
                <p class="title is-4">üì¢ News</p>

                <div class="scrollable-box">
                  <ul class="scrollable-list">
                    <li class="scrollable-item">
                      <time class="info-date">2024-12</time>
                      <span class="info-content">Finished defending my doctoral dissertation.</span>
                    </li>
                  </ul>
                </div>
              </div>

              <div class="column" id="resource">
                <p class="title is-4">üìÇ Resource</p>

                <div class="scrollable-box">
                  <ul class="scrollable-list">
                    <li class="scrollable-item">
                      <time class="info-date">2024-05-31</time>
                      <span class="info-content"><a href="https://pan.baidu.com/s/1NaGtxGWzQSRLmEPukvZnBQ?pwd=abcd">ËßÜËßâ‰∏éËØ≠Ë®Ä‚ÄîÂºÄÊîæËØçÊ±áÊÑüÁü•ÁªºËø∞</a></span>
                    </li>
                  </ul>
                </div>

              </div>
            </div>
          </div>

          <!-- Project section -->
          <div class="box" id="project">
            <p class="title is-4">üíª Project</p>

            <table class="table is-striped is-narrow is-fullwidth">
              <thead>
                <tr>
                  <th>Stars</th>
                  <th>Name</th>
                  <th>Description</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <th><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/lartpang/Hands-on-Docker?style=flat-square"></th>
                  <td><a href="https://github.com/lartpang/Hands-on-Docker"><strong>Hands-on-Docker (‰∏≠Êñá)</strong></a></td>
                  <td>‰∏Ä‰ªΩËØ¶Â∞ΩÁöÑ Docker ‰ΩøÁî®ÊåáÂçó„ÄÇ</td>
                </tr>
                <tr>
                  <th><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/lartpang/awesome-class-activation-map?style=flat-square"></th>
                  <td><a href="https://github.com/lartpang/awesome-class-activation-map"><strong>Awesome-Class-Activation-Map</strong></a></td>
                  <td>An awesome list of papers and tools about the class activation map (CAM) technology.</td>
                </tr>
                <tr>
                  <th><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/lartpang/PyTorchTricks?style=flat-square"></th>
                  <td><a href="https://github.com/lartpang/PyTorchTricks"><strong>PyTorchTricks</strong></a></td>
                  <td>Some tricks of pytorch‚Ä¶</td>
                </tr>
                <tr>
                  <th><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/lartpang/MethodsCmp?style=flat-square"></th>
                  <td><a href="https://github.com/lartpang/MethodsCmp"><strong>MethodsCmp</strong></a></td>
                  <td>A Simple Toolkit for Counting the FLOPs/MACs, Parameters and FPS of Pytorch-based Methods.</td>
                </tr>
                <tr>
                  <th><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/lartpang/PySODEvalToolkit?style=flat-square"></th>
                  <td><a href="https://github.com/lartpang/PySODEvalToolkit"><strong>PySODEvalToolkit</strong></a></td>
                  <td>A Python-based salient object detection and video object segmentation evaluation toolbox.</td>
                </tr>
                <tr>
                  <th><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/lartpang/PySODMetrics?style=flat-square"></th>
                  <td><a href="https://github.com/lartpang/PySODMetrics"><strong>PySODMetrics</strong></a></td>
                  <td>A simple and efficient implementation of SOD metrcis.</td>
                </tr>
                <tr>
                  <th><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/lartpang/PyIRSTDMetrics?style=flat-square"></th>
                  <td><a href="https://github.com/lartpang/PyIRSTDMetrics"><strong>PyIRSTDMetrics</strong></a></td>
                  <td>A simple and efficient implementation for the IRSTD performance analysis.</td>
                </tr>
                <tr>
                  <th><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/lartpang/PyLoss?style=flat-square"></th>
                  <td><a href="https://github.com/lartpang/PyLoss"><strong>PyLoss</strong></a></td>
                  <td>Some loss functions for deeplearning.</td>
                </tr>
                <tr>
                  <th><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/lartpang/OpticalFlowBasedVOS?style=flat-square"></th>
                  <td><a href="https://github.com/lartpang/OpticalFlowBasedVOS"><strong>OpticalFlowBasedVOS</strong></a></td>
                  <td>A simple and efficient codebase for the optical flow based video object segmentation.</td>
                </tr>
                <tr>
                  <th><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/lartpang/CoSaliencyProj?style=flat-square"></th>
                  <td><a href="https://github.com/lartpang/CoSaliencyProj"><strong>CoSaliencyProj</strong></a></td>
                  <td>A project for co-saliency detection. Some codes are borrowed from ICNet (NIPS2020).</td>
                </tr>
                <tr>
                  <th><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/lartpang/RunIt?style=flat-square"></th>
                  <td><a href="https://github.com/lartpang/RunIt"><strong>RunIt</strong></a></td>
                  <td>A simple program scheduler for your code on different devices.</td>
                </tr>
                <tr>
                  <th><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/lartpang/RegisterIt?style=flat-square"></th>
                  <td><a href="https://github.com/lartpang/RegisterIt"><strong>RegisterIt</strong></a></td>
                  <td>Register it: A more flexible register for the DeepLearning project.</td>
                </tr>
                <tr>
                  <th><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/lartpang/mssim.pytorch?style=flat-square"></th>
                  <td><a href="https://github.com/lartpang/mssim.pytorch"><strong>mssim.pytorch</strong></a></td>
                  <td>A better pytorch-based implementation for the mean structural similarity. Differentiable simpler SSIM and MS-SSIM.</td>
                </tr>
                <tr>
                  <th><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/lartpang/tta.pytorch?style=flat-square"></th>
                  <td><a href="https://github.com/lartpang/tta.pytorch"><strong>tta.pytorch</strong></a></td>
                  <td>Test-Time Augmentation library for Pytorch.</td>
                </tr>
                <tr>
                  <th><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/lartpang/YuQueTools?style=flat-square"></th>
                  <td><a href="https://github.com/lartpang/YuQueTools"><strong>YuQueTools</strong></a></td>
                  <td>A simple tool to download your own articles from yuque.</td>
                </tr>
                <tr>
                  <th><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/lartpang/ManageMyAttachments?style=flat-square"></th>
                  <td><a href="https://github.com/lartpang/ManageMyAttachments"><strong>ManageMyAttachments</strong></a></td>
                  <td>Manage the attachments of your own obsidian vault.</td>
                </tr>
              </tbody>
            </table>
          </div>

          <!-- Paper section -->
          <div class="box" id="paper-list">
            <p class="title is-4">üìñ Paper</p>

            <div class="block" id="peerreview">
              <p class="boldsubtitle">Preprint</p>

              <div class="paper-item" id="pbd-jnl">
                <p class="paper-title">Power Battery Detection</p>
                <p class="paper-author">Xiaoqi Zhao, Peiqian Cao, Lihe Zhang, Zonglei Feng, Hanqi Liu, Jiaming Zuo, <span class="author-me">Youwei Pang</span>, Weisi Lin, Georges El Fakhri, Huchuan Lu, Xiaofeng Liu</p>
                <p class="paper-conf">ArXiv 2025</p>

                <p class="paper-info">
                  <span class="tag-link"><a href="https://arxiv.org/abs/2508.07797" target="_blank">[Paper (ArXiv)]</a></span>
                </p>
              </div>

              <div class="paper-item" id="cod-umbd">
                <p class="paper-title">Uncertainty-Masked Bernoulli Diffusion for Camouflaged Object Detection Refinement</p>
                <p class="paper-author">Yuqi Shen, Fengyang Xiao, Sujie Hu, <span class="author-me">Youwei Pang</span>, Yifan Pu, Chengyu Fang, Xiu Li, Chunming He</p>
                <p class="paper-conf">ArXiv 2025</p>

                <p class="paper-info">
                  <span class="tag-link"><a href="https://arxiv.org/abs/2506.10712" target="_blank">[Paper (ArXiv)]</a></span>
                </p>
              </div>

              <div class="paper-item" id="cod-restromem">
                <p class="paper-title">Retrospective Memory for Camouflaged Object Detection</p>
                <p class="paper-author">Chenxi Zhang, Jiayun Wu, Qing Zhang#, Yazhe Zhai, <span class="author-me">Youwei Pang</span>#</p>
                <p class="paper-conf">ArXiv 2025</p>

                <p class="paper-info">
                  <span class="tag-link"><a href="https://arxiv.org/abs/2506.15244" target="_blank">[Paper (ArXiv)]</a></span>
                </p>
              </div>

              <div class="paper-item" id="sam-cdeval">
                <p class="paper-title">Inspiring the Next Generation of Segment Anything Models: Comprehensively Evaluate SAM and SAM 2 with Diverse Prompts Towards Context-Dependent Concepts under Different Scenes</p>
                <p class="paper-author">Xiaoqi Zhao*, <span class="author-me">Youwei Pang</span>*, Shijie Chang*, Yuan Zhao*, Lihe Zhang, Huchuan Lu, Jinsong Ouyang, Georges El Fakhri, Xiaofeng Liu</p>
                <p class="paper-conf">ArXiv 2024</p>

                <p class="paper-info">
                  <span class="tag-link"><a href="https://arxiv.org/abs/2412.01240" target="_blank">[Paper (ArXiv)]</a></span>
                  <span class="tag-link"><a href="https://github.com/lartpang/SAMs-CDConcepts-Eval" target="_blank">[Code (GitHub)]</a></span>
                </p>
              </div>

              <div class="paper-item" id="medical-m2snet">
                <p class="paper-title">M2SNet: Multi-scale in Multi-scale Subtraction Network for Medical Image Segmentation</p>
                <p class="paper-author">Xiaoqi Zhao, Hongpeng Jia, <span class="author-me">Youwei Pang</span>, Long Lv, Feng Tian, Lihe Zhang, Weibing Sun, Huchuan Lu</p>
                <p class="paper-conf">The second place (2/100) in the MICCAI 2022 Challenge: Glaucoma Oct Analysis and Layer Segmentation (GOALS)</p>

                <p class="paper-info">
                  <span class="tag-link"><a href="https://arxiv.org/abs/2303.10894" target="_blank">[Paper (ArXiv)]</a></span>
                </p>
              </div>

            </div>

            <div class="block" id="publication">
              <p class="boldsubtitle">Publication</p>

              <div class="paper-item" id="multimodal-unimerseg">
                <p class="paper-title">UniMRSeg: Unified Modality-Relax Segmentation via Hierarchical Self-Supervised Compensation</p>
                <p class="paper-author">Xiaoqi Zhao, <span class="author-me">Youwei Pang</span>, Chenyang Yu, Lihe Zhang, Huchuan Lu, Shijian Lu, Georges El Fakhri, Xiaofeng Liu</p>
                <p class="paper-conf">The Conference on Neural Information Processing Systems (NeurIPS) 2025</p>

                <p class="paper-info">
                </p>
              </div>

              <div class="paper-item" id="rethinking-irstd">
                <p class="paper-title">Rethinking Evaluation of Infrared Small Target Detection</p>
                <p class="paper-author"><span class="author-me">Youwei Pang</span>, Xiaoqi Zhao, Lihe Zhang, Huchuan Lu, Georges El Fakhri, Xiaofeng Liu, Shijian Lu</p>
                <p class="paper-conf">The Conference on Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track 2025</p>

                <p class="paper-info">
                  <span class="tag-link"><a href="https://arxiv.org/abs/2509.16888" target="_blank">[Paper (ArXiv)]</a></span>
                  <span class="tag-link"><a href="https://github.com/lartpang/PyIRSTDMetrics;" target="_blank">[Code (GitHub)]</a></span>
                </p>
              </div>

              <div class="paper-item" id="cod-cgcod">
                <p class="paper-title">CGCOD: Class-Guided Camouflaged Object Detection</p>
                <p class="paper-author">Chenxi Zhang, Qing Zhang#, Jiayun Wu, <span class="author-me">Youwei Pang</span>#</p>
                <p class="paper-conf">ACM International Conference on Multimedia (ACM MM) 2025</p>

                <p class="paper-info">
                  <span class="tag-link"><a href="https://arxiv.org/abs/2412.18977" target="_blank">[Paper (ArXiv)]</a></span>
                  <span class="tag-link"><a href="https://github.com/bbdjj/CGCOD" target="_blank">[Code (GitHub)]</a></span>
                </p>
              </div>

              <div class="paper-item" id="multimodal-comptr">
                <p class="paper-title">ComPtr: Towards Diverse Bi-source Dense Prediction Tasks via A Simple yet General Complementary Transformer</p>
                <p class="paper-author"><span class="author-me">Youwei Pang</span>, Xiaoqi Zhao, Lihe Zhang, Huchuan Lu</p>
                <p class="paper-conf">Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2025, IEEE</p>

                <p class="paper-info">
                  <span class="tag-info-doi">[DOI:10.1109/TPAMI.2025.3578494]</span> <span class="tag-link"><a href="https://arxiv.org/abs/2307.12349" target="_blank">[Paper (ArXiv)]</a></span>
                  <span class="tag-link"><a href="https://github.com/lartpang/ComPtr" target="_blank">[Code (GitHub)]</a></span>
                </p>
              </div>

              <div class="paper-item" id="cod-see">
                <p class="paper-title">Segment Concealed Objects with Incomplete Supervision</p>
                <p class="paper-author">Chunming He, Kai Li, Yachao Zhang, Ziyun Yang, <span class="author-me">Youwei Pang</span>, Longxiang Tang, Chengyu Fang, Yulun Zhang, Linghe Kong, Xiu Li, Sina Farsiu</p>
                <p class="paper-conf">Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2025, IEEE</p>

                <p class="paper-info">
                  <span class="tag-info-doi">[DOI:10.1109/TPAMI.2025.3576209]</span> <span class="tag-link"><a href="https://ieeexplore.ieee.org/document/11023026/" target="_blank">[Paper (IEEE Xplore)]</a></span>
                  <span class="tag-link"><a href="https://github.com/ChunmingHe/SEE" target="_blank">[Code (GitHub)]</a></span>
                </p>
              </div>

              <div class="paper-item" id="fss-beyoudmask">
                <p class="paper-title">Beyond Mask: Rethinking Guidance Types in Few-shot Segmentation</p>
                <p class="paper-author">Shijie Chang, <span class="author-me">Youwei Pang</span>, Xiaoqi Zhao, Lihe Zhang, Huchuan Lu</p>
                <p class="paper-conf">Pattern Recognition (PR) 2025, Elsevier</p>

                <p class="paper-info">
                  <span class="tag-info-doi">[DOI:10.1016/j.patcog.2025.111635]</span> <span class="tag-link"><a href="https://doi.org/10.1016/j.patcog.2025.111635" target="_blank">[Paper (ScienceDirect)]</a></span>
                  <span class="tag-link"><a href="https://arxiv.org/pdf/2407.11503" target="_blank">[Paper (ArXiv)]</a></span>
                  <span class="tag-link"><a href="https://github.com/DUT-CSJ/FoundationFSS" target="_blank">[Code (GitHub)]</a></span>
                </p>
              </div>

              <div class="paper-item" id="cod-ovcos">
                <p class="paper-title">Open-Vocabulary Camouflaged Object Segmentation</p>
                <p class="paper-author"><span class="author-me">Youwei Pang</span>*, Xiaoqi Zhao*, Jiaming Zuo, Lihe Zhang, Huchuan Lu</p>
                <p class="paper-conf">European Conference on Computer Vision (ECCV) 2024, Springer</p>

                <p class="paper-info">
                  <span class="tag-info-doi">[DOI:10.1007/978-3-031-72970-6_27]</span> <span class="tag-link"><a href="https://doi.org/10.1007/978-3-031-72970-6_27" target="_blank">[Paper (Springer Nature Link)]</a></span>
                  <span class="tag-link"><a href="https://arxiv.org/abs/2311.11241" target="_blank">[Paper (ArXiv)]</a></span>
                  <span class="tag-link"><a href="https://github.com/lartpang/OVCamo" target="_blank">[Code (GitHub)]</a></span>
                </p>
              </div>

              <div class="paper-item" id="cod-zoomnext">
                <p class="paper-title">ZoomNeXt: A Unified Collaborative Pyramid Network for Camouflaged Object Detection</p>
                <p class="paper-author"><span class="author-me">Youwei Pang</span>*, Xiaoqi Zhao*, Tian-Zhu Xiang*, Lihe Zhang, Huchuan Lu</p>
                <p class="paper-conf">Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2024, IEEE</p>

                <p class="paper-info">
                  <span class="tag-info-ei">[EI:20242616353583]</span>
                  <span class="tag-info-doi">[DOI:10.1109/TPAMI.2024.3417329]</span>
                  <span class="tag-link"><a href="https://arxiv.org/abs/2310.20208" target="_blank">[Paper (ArXiv)]</a></span>
                  <span class="tag-link"><a href="https://github.com/lartpang/ZoomNeXt" target="_blank">[Code (GitHub)]</a></span>
                </p>
              </div>

              <div class="paper-item" id="unified-spider">
                <p class="paper-title">Spider: A Unified Framework for Context-dependent Concept Understanding</p>
                <p class="paper-author">Xiaoqi Zhao*, <span class="author-me">Youwei Pang</span>*, Wei Ji*, Baicheng Sheng, Jiaming Zuo, Lihe Zhang, Huchuan Lu</p>
                <span class="paper-conf">International Conference on Machine Learning (ICML) 2024, PMLR</span>

                <p class="paper-info">
                  <span class="tag-info-ei">[EI:20243817053162]</span>
                  <span class="tag-link"><a href="https://arxiv.org/abs/2405.01002" target="_blank">[Paper (ArXiv)]</a></span>
                  <span class="tag-link"><a href="https://github.com/Xiaoqi-Zhao-DLUT/Spider-UniCDSeg" target="_blank">[Code]</a></span>
                </p>
              </div>

              <div class="paper-item" id="pbd-conf">
                <p class="paper-title">Towards Automatic Power Battery Detection: New Challenge, Benchmark Dataset and Baseline</p>
                <p class="paper-author">Xiaoqi Zhao*, <span class="author-me">Youwei Pang</span>*, Zhenyu Chen, Qian Yu, Lihe Zhang, Hanqi Liu, Jiaming Zuo</p>
                <p class="paper-conf">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2024, IEEE</p>

                <p class="paper-info">
                  <span class="tag-info-ei">[EI:20244317260998]</span>
                  <span class="tag-info-doi">[DOI:10.1109/CVPR52733.2024.02079]</span>
                  <span class="tag-link"><a href="https://arxiv.org/abs/2312.02528" target="_blank">[Paper (ArXiv)]</a></span>
                  <span class="tag-link"><a href="https://xiaoqi-zhao-dlut.github.io/xray_pbd_cvpr2024/" target="_blank">[Project]</a></span>
                  <span class="tag-link"><a href="https://xiaoqi-zhao-dlut.github.io/xray_pbd_cvpr2024/" target="_blank">[Â∑•Ê∫ê‰∏â‰ªüÂÖ¨‰ºóÂè∑]</a></span>
                  <span class="tag-link"><a href="https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD" target="_blank">[Code]</a></span>
                </p>
              </div>

              <div class="paper-item" id="dis-mvanet">
                <p class="paper-title">Multi-view Aggregation Network for Dichotomous Image Segmentation</p>
                <p class="paper-author">Qian Yu*, Xiaoqi Zhao*, <span class="author-me">Youwei Pang</span>*, Lihe Zhang, Huchuan Lu</p>
                <p class="paper-conf">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2024, IEEE <span class="paper-oral has-text-danger">Highlight</span></p>

                <p class="paper-info">
                  <span class="tag-info-ei">[EI:20244317260270]</span>
                  <span class="tag-info-doi">[DOI:10.1109/CVPR52733.2024.00376]</span>
                  <span class="tag-link"><a href="https://arxiv.org/abs/2404.07445" target="_blank">[Paper (ArXiv)]</a></span>
                  <span class="tag-link"><a href="https://github.com/qianyu-dlut/MVANet" target="_blank">[Code]</a></span>
                </p>
              </div>

              <div class="paper-item" id="gatenet-jnl">
                <p class="paper-title">Towards Diverse Binary Segmentation via A Simple yet General Gated Network</p>
                <p class="paper-author">Xiaoqi Zhao, <span class="author-me">Youwei Pang</span>, Lihe Zhang, Huchuan Lu, Lei Zhang</p>
                <p class="paper-conf">International Journal of Computer Vision (IJCV) 2024, Springer</p>

                <p class="paper-info">
                  <span class="tag-info-ei">[EI:20241916055804]</span>
                  <span class="tag-info-wos">[WOS:001215379300003]</span>
                  <span class="tag-info-doi">[DOI:10.1007/s11263-024-02058-y]</span>
                  <span class="tag-link"><a href="https://arxiv.org/abs/2303.10396" target="_blank">[Paper (ArXiv)]</a></span>
                  <span class="tag-link"><a href="https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency" target="_blank">[Code]</a></span>
                </p>
              </div>

              <div class="paper-item" id="zsvos-amsp">
                <p class="paper-title">Adaptive Multi-source Predictor for Zero-shot Video Object Segmentation</p>
                <p class="paper-author">Xiaoqi Zhao, Shijie Chang, <span class="author-me">Youwei Pang</span>, Jiaxing Yang, Lihe Zhang, Huchuan Lu</p>
                <p class="paper-conf">International Journal of Computer Vision (IJCV) 2024, Springer</p>

                <p class="paper-info">
                  <span class="tag-info-ei">[EI:20241015709995]</span>
                  <span class="tag-info-wos">[WOS:001176531200002]</span>
                  <span class="tag-info-doi">[DOI:10.1007/s11263-024-02024-8]</span>
                  <span class="tag-link"><a href="https://arxiv.org/abs/2303.10383" target="_blank">[Paper (ArXiv)]</a></span>
                </p>
              </div>

              <div class="paper-item" id="rawsd-sara">
                <p class="paper-title">Adaptive Illumination Mapping for Shadow Detection in Raw Images</p>
                <p class="paper-author">Jiayu Sun, Ke Xu, <span class="author-me">Youwei Pang</span>, Lihe Zhang, Huchuan Lu, Gerhard Hancke, Rynson Lau</p>
                <p class="paper-conf">IEEE/CVF International Conference on Computer Vision (ICCV) 2023, IEEE</p>

                <p class="paper-info">
                  <span class="tag-info-ei">[EI:20240915634899]</span>
                  <span class="tag-info-wos">[WOS:001169499005013]</span>
                  <span class="tag-info-doi">[DOI:10.1109/ICCV51070.2023.01167]</span>
                  <span class="tag-link"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Sun_Adaptive_Illumination_Mapping_for_Shadow_Detection_in_Raw_Images_ICCV_2023_paper.html" target="_blank">[Paper (CVF)]</a></span>
                  <span class="tag-link"><a href="https://github.com/jiayusun/SARA" target="_blank">[Code]</a></span>
                </p>
              </div>

              <div class="paper-item" id="multimodal-caver">
                <p class="paper-title">CAVER: Cross-Modal View-Mixed Transformer for Bi-Modal Salient Object Detection</p>
                <p class="paper-author"><span class="author-me">Youwei Pang</span>, Xiaoqi Zhao, Lihe Zhang, Huchuan Lu</p>
                <p class="paper-conf">Transactions on Image Processing (TIP) 2023, IEEE</p>

                <p class="paper-info">
                  <span class="tag-info-ei">[EI:20230613542686]</span>
                  <span class="tag-info-wos">[WOS:000922870200004] (üèÜ ESI Highly Cited Paper)</span>
                  <span class="tag-info-doi">[DOI:10.1109/TIP.2023.3234702]</span>
                  <span class="tag-link"><a href="https://arxiv.org/abs/2112.02363" target="_blank">[Paper (ArXiv)]</a></span>
                  <span class="tag-link"><a href="https://ieeexplore.ieee.org/document/10015667" target="_blank">[Paper (IEEE)]</a></span>
                  <span class="tag-link"><a href="https://github.com/lartpang/CAVER" target="_blank">[Code]</a></span>
                  <span class="tag-link"><a href="/docs/caver.html" target="_blank">[Project]</a></span>
                </p>
              </div>

              <div class="paper-item" id="multimodal-jlsdc">
                <p class="paper-title">Joint Learning of Salient Object Detection, Depth Estimation and Contour Extraction</p>
                <p class="paper-author">Xiaoqi Zhao, <span class="author-me">Youwei Pang</span>, Lihe Zhang, Huchuan Lu</p>
                <p class="paper-conf">Transactions on Image Processing (TIP) 2022, IEEE</p>

                <p class="paper-info">
                  <span class="tag-info-ei">[EI:20225113272379]</span>
                  <span class="tag-info-wos">[WOS:000892917400002]</span>
                  <span class="tag-info-doi">[DOI:10.1109/TIP.2022.3222641]</span>
                  <span class="tag-link"><a href="https://arxiv.org/abs/2203.04895" target="_blank">[Paper]</a></span>
                  <span class="tag-link"><a href="https://github.com/Xiaoqi-Zhao-DLUT/MMFT" target="_blank">[Code]</a></span>
                </p>
              </div>

              <div class="paper-item" id="cod-zoomnet">
                <p class="paper-title">Zoom In and Out: A Mixed-scale Triplet Network for Camouflaged Object Detection</p>
                <p class="paper-author"><span class="author-me">Youwei Pang</span>*, Xiaoqi Zhao*, Tian-zhu Xiang, Lihe Zhang, Huchuan Lu</p>
                <p class="paper-conf">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2022, IEEE</p>

                <p class="paper-info">
                  <span class="tag-info-ei">[EI:20224613119658]</span>
                  <span class="tag-info-wos">[WOS:000867754202041]</span>
                  <span class="tag-info-doi">[DOI:10.1109/CVPR52688.2022.00220]</span>
                  <span class="tag-link"><a href="https://arxiv.org/abs/2203.02688" target="_blank">[Paper]</a></span>
                  <span class="tag-link"><a href="https://github.com/lartpang/ZoomNet" target="_blank">[Code]</a></span>
                  <span class="tag-link"><a href="/docs/zoomnet.html" target="_blank">[Project]</a></span>
                </p>
              </div>

              <div class="paper-item" id="sslsod">
                <p class="paper-title">Self-Supervised Pretraining for RGB-D Salient Object Detection</p>
                <p class="paper-author">Xiaoqi Zhao, <span class="author-me">Youwei Pang</span>, Lihe Zhang, Huchuan Lu, Xiang Ruan</p>
                <p class="paper-conf">Association for the Advancement of Artificial Intelligence (AAAI) 2022</p>

                <p class="paper-info">
                  <span class="tag-info-ei">[EI:20230713571733]</span>
                  <span class="tag-info-wos">[WOS:000893636203061]</span>
                  <span class="tag-link"><a href="https://arxiv.org/abs/2101.12482" target="_blank">[Paper]</a></span>
                  <span class="tag-link"><a href="https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247591348&idx=2&sn=34a95dbbd14b7adfd508f12899ac27a2&chksm=ec1d884ddb6a015b71612bf16227bed8c69db4074585f712c1dc7dceb7859641088441ff9753&mpshare=1&scene=1&srcid=0112O4ivFbyChzTTH4vKU91t&sharer_sharetime=1646716025907&sharer_shareid=0ffc6ac03af605267e92344350efdb83&exportkey=AxNWDL6LKSsLK6MxSNkSF88%3D&acctmode=0&pass_ticket=TXKuWY6yeluRhUKTt0pk10ycuy%2BMsyJV6%2BXdxFjTtusuYyJMVPywg38icEXhKktM&wx_header=0#rd" target="_blank">[Slide&ÊûÅÂ∏ÇÂπ≥Âè∞Êé®ÈÄÅ]</a></span>
                  <span class="tag-link"><a href="https://github.com/Xiaoqi-Zhao-DLUT/SSLSOD" target="_blank">[Code]</a></span>
                </p>
              </div>

              <div class="paper-item" id="zsvos-msfaps">
                <p class="paper-title">Multi-Source Fusion and Automatic Predictor Selection for Zero-Shot Video Object Segmentation</p>
                <p class="paper-author">Xiaoqi Zhao, <span class="author-me">Youwei Pang</span>, Jiaxing Yang, Lihe Zhang, Huchuan Lu</p>
                <p class="paper-conf">ACM International Conference on Multimedia (ACM MM) 2021 <span class="paper-oral has-text-danger">Oral</span></p>

                <p class="paper-info">
                  <span class="tag-info-ei">[EI:20214711200241]</span>
                  <span class="tag-info-wos">[WOS:001147786902077]</span>
                  <span class="tag-link"><a href="https://arxiv.org/abs/2108.05076" target="_blank">[Paper]</a></span>
                  <span class="tag-link"><a href="https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247570266&idx=2&sn=547fbfdeaa02b0ff83dec3b24a58a632&chksm=ec1d3ea3db6ab7b5cfbe7ba2fffcd4460584f1e2743d8265562757e508030bf049d1c3140d3c&mpshare=1&scene=23&srcid=0902BUt8U9MY8k2IvY500f71&sharer_sharetime=1630565724193&sharer_shareid=5c55e87df338791d997e8905ad2ebfe0#rd" target="_blank">[Slide&ÊûÅÂ∏ÇÂπ≥Âè∞Êé®ÈÄÅ]</a></span>
                  <span class="tag-link"><a href="https://github.com/Xiaoqi-Zhao-DLUT/Multi-Source-APS-ZVOS" target="_blank">[Code]</a></span>
                </p>
              </div>

              <div class="paper-item" id="multimodal-hdfnet">
                <p class="paper-title">Hierarchical Dynamic Filtering Network for RGB-D Salient Object Detection</p>
                <p class="paper-author"><span class="author-me">Youwei Pang</span>, Lihe Zhang, Xiaoqi Zhao, Huchuan Lu</p>
                <p class="paper-conf">European Conference on Computer Vision (ECCV) 2020, Springer</p>

                <p class="paper-info">
                  <span class="tag-info-ei">[EI:20205009617977]</span>
                  <span class="tag-info-doi">[DOI:10.1007/978-3-030-58595-2_15]</span>
                  <span class="tag-link"><a href="https://arxiv.org/abs/2007.06227" target="_blank">[Paper]</a></span>
                  <span class="tag-link"><a href="https://github.com/lartpang/HDFNet/blob/master/LongPresentation.pdf" target="_blank">[Slide]</a></span>
                  <span class="tag-link"><a href="https://github.com/lartpang/HDFNet" target="_blank">[Code]</a></span>
                </p>
              </div>

              <div class="paper-item" id="gatenet-conf">
                <p class="paper-title">Suppress and Balance: A Simple Gated Network for Salient Object Detection</p>
                <p class="paper-author">Xiaoqi Zhao*, <span class="author-me">Youwei Pang</span>*, Lihe Zhang, Huchuan Lu, Lei Zhang</p>
                <p class="paper-conf">European Conference on Computer Vision (ECCV) 2020, Springer <span class="paper-oral has-text-danger">Oral</span></p>

                <p class="paper-info">
                  <span class="tag-info-ei">[EI:20205009597084]</span>
                  <span class="tag-info-doi">[DOI:10.1007/978-3-030-58536-5_3]</span>
                  <span class="tag-link"><a href="https://arxiv.org/abs/2007.08074" target="_blank">[Paper]</a></span>
                  <span class="tag-link"><a href="https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency/blob/master/2852.pdf" target="_blank">[Slide]</a></span>
                  <span class="tag-link"><a href="https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency" target="_blank">[Code]</a></span>
                </p>
              </div>

              <div class="paper-item" id="multimodal-danet">
                <p class="paper-title">A Single Stream Network for Robust and Real-time RGB-D Salient Object Detection</p>
                <p class="paper-author">Xiaoqi Zhao, Lihe Zhang, <span class="author-me">Youwei Pang</span>, Huchuan Lu, Lei Zhang</p>
                <p class="paper-conf">European Conference on Computer Vision (ECCV) 2020, Springer</p>

                <p class="paper-info">
                  <span class="tag-info-ei">[EI:20205009601094]</span>
                  <span class="tag-info-doi">[DOI:10.1007/978-3-030-58542-6_39]</span>
                  <span class="tag-link"><a href="https://arxiv.org/abs/2007.06811" target="_blank">[Paper]</a></span>
                  <span class="tag-link"><a href="https://github.com/Xiaoqi-Zhao-DLUT/DANet-RGBD-Saliency" target="_blank">[Code]</a></span>
                </p>
              </div>

              <div class="paper-item" id="sod-minet">
                <p class="paper-title">Multi-scale Interactive Network for Salient Object Detection</p>
                <p class="paper-author"><span class="author-me">Youwei Pang</span>*, Xiaoqi Zhao*, Lihe Zhang, Huchuan Lu</p>
                <p class="paper-conf">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2020, IEEE</p>

                <p class="paper-info">
                  <span class="tag-info-ei">[EI:20204409431574]</span>
                  <span class="tag-info-wos">[WOS:001309199902028]</span>
                  <span class="tag-info-doi">[DOI:10.1109/CVPR42600.2020.00943]</span>
                  <span class="tag-link"><a href="https://arxiv.org/abs/2007.09062" target="_blank">[Paper]</a></span>
                  <span class="tag-link"><a href="https://github.com/lartpang/MINet" target="_blank">[Code]</a></span>
                </p>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Footer -->
  <footer class="footer py-4">
    <div class="content has-text-centered">
      Last updated at <time class="info-date">2025-9-24</time>
      ¬© Youwei Pang. All rights reserved.
    </div>
  </footer>
</body>

<script>
  // ÂÆö‰πâÊò†Â∞ÑÂÖ≥Á≥ª
  const classMap = {
    'boldsubtitle': ['has-text-weight-bold', 'is-size-5'],
    'paper-title': ['has-text-weight-bold', 'is-size-6'],
    'paper-author': ['is-italic', 'is-size-7'],
    'paper-conf': ['has-text-weight-semibold', 'is-size-7'],
    'paper-info': ['is-size-7'],
    'paper-item': ['card', 'my-2'],
    'author-me': ['has-text-info', 'has-text-weight-bold'],
    'tag-link': ['tag', 'is-link', 'is-light', 'has-text-weight-bold'],
    'tag-info-ei': ['tag', 'is-info', 'has-text-weight-bold'],
    'tag-info-wos': ['tag', 'is-primary', 'has-text-weight-bold'],
    'tag-info-doi': ['tag', 'is-black', 'has-text-weight-bold'],
  };

  // ÊõøÊç¢Á±ªÂà´Âêç‰∏∫ÂÆûÈôÖÁöÑÁ±ªÂÆö‰πâ
  function replaceClassAliases() {
    // ÈÅçÂéÜÊò†Â∞Ñ‰∏≠ÁöÑÊØè‰∏™Âà´Âêç
    Object.keys(classMap).forEach(alias => {
      // Êü•Êâæ‰ΩøÁî®Âà´ÂêçÁöÑÊâÄÊúâÂÖÉÁ¥†
      document.querySelectorAll('.' + alias).forEach(element => {
        // ÁßªÈô§Âà´ÂêçÁ±ª
        element.classList.remove(alias);
        // Ê∑ªÂä†Êò†Â∞ÑÂà∞ÁöÑÂÆûÈôÖÁ±ª
        classMap[alias].forEach(actualClass => {
          element.classList.add(actualClass);
        });
      });
    });
  }

  // navbar active, https://bulma.io/documentation/components/navbar/#navbar-menu
  function activateNavbarMenu() {
    // Get all "navbar-burger" elements
    const $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);
    // Add a click event on each of them
    $navbarBurgers.forEach(el => {
      el.addEventListener('click', () => {
        // Get the target from the "data-target" attribute
        const target = el.dataset.target;
        const $target = document.getElementById(target);
        // Toggle the "is-active" class on both the "navbar-burger" and the "navbar-menu"
        el.classList.toggle('is-active');
        $target.classList.toggle('is-active');
      });
    });
  }

  function decryptName(encryptedName) {
    return encryptedName.split('').map(char =>
      String.fromCharCode(char.charCodeAt(0) - 5)
    ).join('');
  }

  // ÈÄêÊ≠•ÊòæÁ§∫Ëß£Á†ÅÂêéÁöÑÂêçÁß∞
  function startDecoding() {
    const nameElement = document.getElementById("email");
    const encryptedName = nameElement.textContent;
    const realName = decryptName(encryptedName);
    let currentIndex = 0;

    const interval = setInterval(() => {
      if (currentIndex < realName.length) {
        nameElement.textContent = realName.substring(0, currentIndex + 1) + encryptedName.substring(currentIndex + 1);
        currentIndex++;
      } else {
        clearInterval(interval); // ÂÅúÊ≠¢ÂÆöÊó∂Âô®
        nameElement.style.color = "black";
        nameElement.style.textDecoration = "none";
        nameElement.onclick = null; // ÁßªÈô§ÁÇπÂáª‰∫ã‰ª∂
      }
    }, 100); // ÊØè100ÊØ´ÁßíÊòæÁ§∫‰∏Ä‰∏™Â≠óÁ¨¶
  }

  // ÂàõÂª∫Á≤íÂ≠ê
  function createParticles(count, width, height, colors) {
    const particles = [];
    for (let i = 0; i < count; i++) {
      particles.push({
        x: Math.random() * width,
        y: Math.random() * height,
        size: Math.random() * 3 + 1,
        color: colors[Math.floor(Math.random() * colors.length)],
        speedX: Math.random() * 2 - 1,
        speedY: Math.random() * 2 - 1
      });
    }
    return particles;
  }

  // Êõ¥Êñ∞Á≤íÂ≠ê‰ΩçÁΩÆ
  function updateParticlePosition(p, canvasWidth, canvasHeight) {
    p.x += p.speedX;
    p.y += p.speedY;

    if (p.x < 0 || p.x > canvasWidth) p.speedX *= -1;
    if (p.y < 0 || p.y > canvasHeight) p.speedY *= -1;
  }

  // ÁªòÂà∂Á≤íÂ≠ê
  function drawParticle(ctx, p) {
    ctx.beginPath();
    ctx.arc(p.x, p.y, p.size, 0, Math.PI * 2);
    ctx.fillStyle = p.color;
    ctx.fill();
  }

  // ÁªòÂà∂Á≤íÂ≠êÈó¥ÁöÑËøûÁ∫ø
  function drawParticleConnections(ctx, particles, index) {
    for (let j = index + 1; j < particles.length; j++) {
      const p2 = particles[j];
      const distance = Math.sqrt(Math.pow(particles[index].x - p2.x, 2) +
        Math.pow(particles[index].y - p2.y, 2));

      if (distance < 100) {
        ctx.beginPath();
        ctx.strokeStyle = particles[index].color;
        ctx.globalAlpha = 1 - distance / 100;
        ctx.lineWidth = 0.5;
        ctx.moveTo(particles[index].x, particles[index].y);
        ctx.lineTo(p2.x, p2.y);
        ctx.stroke();
        ctx.globalAlpha = 1;
      }
    }
  }

  // ÂàùÂßãÂåñÁ≤íÂ≠êÂä®Áîª
  function initParticleAnimation() {
    const canvas = document.getElementById('particles-canvas');
    const ctx = canvas.getContext('2d');

    // ËÆæÁΩÆcanvasÂ∞∫ÂØ∏
    canvas.width = window.innerWidth;
    canvas.height = window.innerHeight;

    // Á≤íÂ≠êÈÖçÁΩÆ
    const colors = ['#1C80AA', '#3B5174', '#DE7B56', '#FDD2BE', '#22B0C3'];
    const particleCount = Math.floor(window.innerWidth / 10);
    const particles = createParticles(particleCount, canvas.width, canvas.height, colors);

    // Âä®ÁîªÂæ™ÁéØ
    function animate() {
      ctx.clearRect(0, 0, canvas.width, canvas.height);

      particles.forEach((p, index) => {
        updateParticlePosition(p, canvas.width, canvas.height);
        drawParticle(ctx, p);
        drawParticleConnections(ctx, particles, index);
      });

      requestAnimationFrame(animate);
    }

    // Â§ÑÁêÜÁ™óÂè£Â§ßÂ∞èÂèòÂåñ
    window.addEventListener('resize', () => {
      canvas.width = window.innerWidth;
      canvas.height = window.innerHeight;
    });

    animate();
  }

  // Âú®ÊñáÊ°£Âä†ËΩΩÂÆåÊàêÂêéÊâßË°åËá™ÂÆö‰πâÂäüËÉΩ
  document.addEventListener('DOMContentLoaded', () => {
    replaceClassAliases();
    activateNavbarMenu();
    initParticleAnimation();
  });
</script>

</html>
